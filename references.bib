@article{arriba2024,
  author    = {Arriba-P{\'e}rez, F. and Garc{\'i}a-M{\'e}ndez, S.},
  title     = {Leveraging LLMs for real-time mental health predictions},
  journal   = {Arabian Journal for Science and Engineering},
  year      = {2024},
  doi       = {10.1007/s13369-024-09508-2}
}

@article{gbollie2023,
  author    = {Gbollie, E. F. and Bantjes, J. and Jarvis, L.},
  title     = {Intention to use digital mental health solutions},
  journal   = {Digital Health},
  year      = {2023},
  volume    = {9},
  pages     = {1--19},
  doi       = {10.1177/20552076231216559}
}

@article{sejnowski2023,
  author    = {Sejnowski, T. J.},
  title     = {Large Language Models and the Reverse Turing Test},
  journal   = {Neural Computation},
  year      = {2023},
  volume    = {35},
  pages     = {309--342},
  doi       = {10.1162/neco_a_01563}
}

@misc{na2024,
  author    = {Na, H.},
  title     = {CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy},
  howpublished = {arXiv},
  year      = {2024},
  doi       = {10.48550/arXiv.2403.16008}
}

@article{stade2024,
  author    = {Stade, E. C. and Stirman, S. W. and Ungar, L. H.},
  title     = {Large language models could change the future of behavioral healthcare},
  journal   = {npj Mental Health Research},
  year      = {2024},
  volume    = {3},
  pages     = {12},
  doi       = {10.1038/s44184-024-00056-z}
}

@inproceedings{xu2024,
  author    = {Xu, X. and Yao, B. and Dong, Y.},
  title     = {Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data},
  booktitle = {Proceedings of ACM Interact},
  year      = {2024},
  doi       = {10.1145/3643540}
}

@article{li2023,
  author    = {Li, H. and Zhang, R. and Kraut, R. E.},
  title     = {Systematic review and meta-analysis of AI-based conversational agents for mental health},
  journal   = {npj Digital Medicine},
  year      = {2023},
  volume    = {6},
  pages     = {236},
  doi       = {10.1038/s41746-023-00979-5}
}

@article{mcgorry2025,
  author    = {McGorry, P. and Gunasiri, H. and Mei, C. and Rice, S. and Gao, C. X.},
  title     = {The youth mental health crisis: analysis and solutions},
  journal   = {Frontiers in Psychiatry},
  year      = {2025},
  volume    = {15},
  pages     = {1517533},
  doi       = {10.3389/fpsyt.2024.1517533}
}

@article{greco2023,
  author    = {Greco, C. M. and Simeri, A. and Tagarelli, A.},
  title     = {Transformer-based language models for mental health issues: A survey},
  journal   = {Pattern Recognition Letters},
  year      = {2023},
  volume    = {167},
  pages     = {204--211},
  doi       = {10.1016/j.patrec.2023.02.016}
}


@ARTICLE{Volkmer2024-xw,
  title    = "Large language models in psychiatry: Opportunities and challenges",
  author   = "Volkmer, Sebastian and Meyer-Lindenberg, Andreas and Schwarz,
              Emanuel",
  abstract = "The ability of Large Language Models (LLMs) to analyze and
              respond to freely written text is causing increasing excitement
              in the field of psychiatry; the application of such models
              presents unique opportunities and challenges for psychiatric
              applications. This review article seeks to offer a comprehensive
              overview of LLMs in psychiatry, their model architecture,
              potential use cases, and clinical considerations. LLM frameworks
              such as ChatGPT/GPT-4 are trained on huge amounts of text data
              that are sometimes fine-tuned for specific tasks. This opens up a
              wide range of possible psychiatric applications, such as
              accurately predicting individual patient risk factors for
              specific disorders, engaging in therapeutic intervention, and
              analyzing therapeutic material, to name a few. However, adoption
              in the psychiatric setting presents many challenges, including
              inherent limitations and biases in LLMs, concerns about
              explainability and privacy, and the potential damage resulting
              from produced misinformation. This review covers potential
              opportunities and limitations and highlights potential
              considerations when these models are applied in a real-world
              psychiatric context.",
  journal  = "Psychiatry Res",
  volume   =  339,
  pages    = "116026",
  month    =  jun,
  year     =  2024,
  address  = "Ireland",
  keywords = "BERT; GPT; Hallucination; Llama; Medical question answering;
              PaLM; Therapy; Transformer",
  language = "en"
}

@ARTICLE{Guo2024-ba,
  title    = "Large Language Models for Mental Health Applications: Systematic
              Review",
  author   = "Guo, Zhijun and Lai, Alvina and Thygesen, Johan H and Farrington,
              Joseph and Keen, Thomas and Li, Kezhi",
  abstract = "BACKGROUND: Large language models (LLMs) are advanced artificial
              neural networks trained on extensive datasets to accurately
              understand and generate natural language. While they have
              received much attention and demonstrated potential in digital
              health, their application in mental health, particularly in
              clinical settings, has generated considerable debate. OBJECTIVE:
              This systematic review aims to critically assess the use of LLMs
              in mental health, specifically focusing on their applicability
              and efficacy in early screening, digital interventions, and
              clinical settings. By systematically collating and assessing the
              evidence from current studies, our work analyzes models,
              methodologies, data sources, and outcomes, thereby highlighting
              the potential of LLMs in mental health, the challenges they
              present, and the prospects for their clinical use. METHODS:
              Adhering to the PRISMA (Preferred Reporting Items for Systematic
              Reviews and Meta-Analyses) guidelines, this review searched 5
              open-access databases: MEDLINE (accessed by PubMed), IEEE Xplore,
              Scopus, JMIR, and ACM Digital Library. Keywords used were (mental
              health OR mental illness OR mental disorder OR psychiatry) AND
              (large language models). This study included articles published
              between January 1, 2017, and April 30, 2024, and excluded
              articles published in languages other than English. RESULTS: In
              total, 40 articles were evaluated, including 15 (38\%) articles
              on mental health conditions and suicidal ideation detection
              through text analysis, 7 (18\%) on the use of LLMs as mental
              health conversational agents, and 18 (45\%) on other applications
              and evaluations of LLMs in mental health. LLMs show good
              effectiveness in detecting mental health issues and providing
              accessible, destigmatized eHealth services. However, assessments
              also indicate that the current risks associated with clinical use
              might surpass their benefits. These risks include inconsistencies
              in generated text; the production of hallucinations; and the
              absence of a comprehensive, benchmarked ethical framework.
              CONCLUSIONS: This systematic review examines the clinical
              applications of LLMs in mental health, highlighting their
              potential and inherent risks. The study identifies several
              issues: the lack of multilingual datasets annotated by experts,
              concerns regarding the accuracy and reliability of generated
              content, challenges in interpretability due to the ``black box''
              nature of LLMs, and ongoing ethical dilemmas. These ethical
              concerns include the absence of a clear, benchmarked ethical
              framework; data privacy issues; and the potential for
              overreliance on LLMs by both physicians and patients, which could
              compromise traditional medical practices. As a result, LLMs
              should not be considered substitutes for professional mental
              health services. However, the rapid development of LLMs
              underscores their potential as valuable clinical aids,
              emphasizing the need for continued research and development in
              this area. TRIAL REGISTRATION: PROSPERO CRD42024508617;
              https://www.crd.york.ac.uk/prospero/display\_record.php?RecordID=508617.",
  journal  = "JMIR Ment Health",
  volume   =  11,
  pages    = "e57400",
  month    =  oct,
  year     =  2024,
  address  = "Canada",
  keywords = "BERT; Bidirectional Encoder Representations from Transformers;
              ChatGPT; digital health care; large language models; mental
              health",
  language = "en"
}

@ARTICLE{Iwaya2022-ut,
  title    = "On the privacy of mental health apps",
  author   = "Iwaya, Leonardo Horn and Babar, M Ali and Rashid, Awais and
              Wijayarathna, Chamila",
  abstract = "An increasing number of mental health services are now offered
              through mobile health (mHealth) systems, such as in mobile
              applications (apps). Although there is an unprecedented growth in
              the adoption of mental health services, partly due to the
              COVID-19 pandemic, concerns about data privacy risks due to
              security breaches are also increasing. Whilst some studies have
              analyzed mHealth apps from different angles, including security,
              there is relatively little evidence for data privacy issues that
              may exist in mHealth apps used for mental health services, whose
              recipients can be particularly vulnerable. This paper reports an
              empirical study aimed at systematically identifying and
              understanding data privacy incorporated in mental health apps. We
              analyzed 27 top-ranked mental health apps from Google Play Store.
              Our methodology enabled us to perform an in-depth privacy
              analysis of the apps, covering static and dynamic analysis, data
              sharing behaviour, server-side tests, privacy impact assessment
              requests, and privacy policy evaluation. Furthermore, we mapped
              the findings to the LINDDUN threat taxonomy, describing how
              threats manifest on the studied apps. The findings reveal
              important data privacy issues such as unnecessary permissions,
              insecure cryptography implementations, and leaks of personal data
              and credentials in logs and web requests. There is also a high
              risk of user profiling as the apps' development do not provide
              foolproof mechanisms against linkability, detectability and
              identifiability. Data sharing among 3rd-parties and advertisers
              in the current apps' ecosystem aggravates this situation. Based
              on the empirical findings of this study, we provide
              recommendations to be considered by different stakeholders of
              mHealth apps in general and apps developers in particular. We
              conclude that while developers ought to be more knowledgeable in
              considering and addressing privacy issues, users and health
              professionals can also play a role by demanding privacy-friendly
              apps.",
  journal  = "Empirical Software Engineering",
  volume   =  28,
  number   =  1,
  pages    = "2",
  month    =  nov,
  year     =  2022
}

@article{weizenbaum1966,
  author    = {Weizenbaum, J.},
  title     = {ELIZA—a computer program for the study of natural language communication between man and machine},
  journal   = {Communications of the ACM},
  volume    = {9},
  number    = {1},
  pages     = {36--45},
  year      = {1966}
}

@article{hubel1959,
  author    = {Hubel, D. H. and Wiesel, T. N.},
  title     = {Receptive fields of single neurones in the cat's striate cortex},
  journal   = {Journal of Physiology},
  volume    = {148},
  pages     = {574--591},
  year      = {1959}
}

@article{fukushima1969,
  author    = {Fukushima, K.},
  title     = {Visual feature extraction by a multilayered network of analog threshold elements},
  journal   = {IEEE Transactions on Systems Science and Cybernetics},
  volume    = {5},
  number    = {4},
  pages     = {322--333},
  year      = {1969}
}

@article{gers2000,
  author    = {Gers, F. A. and Schmidhuber, J. and Cummins, F.},
  title     = {Learning to forget: Continual prediction with LSTM},
  journal   = {Neural Computation},
  volume    = {12},
  number    = {10},
  pages     = {2451--2471},
  year      = {2000}
}

@article{dosovitskiy2020,
  author    = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and Uszkoreit, J. and Houlsby, N.},
  title     = {An image is worth 16x16 words: Transformers for image recognition at scale},
  journal   = {arXiv preprint},
  volume    = {arXiv:2010.11929},
  year      = {2020},
  note      = {\url{https://arxiv.org/abs/2010.11929}}
}

@article{radford2019,
  author    = {Radford, A. and Wu, J. and Child, R. and Luan, D. and Amodei, D. and Sutskever, I.},
  title     = {Language models are unsupervised multitask learners},
  journal   = {OpenAI Blog},
  volume    = {1},
  number    = {8},
  pages     = {9},
  year      = {2019}
}

@inproceedings{jelinek1980,
  author    = {Jelinek, F. and Mercer, R. L.},
  title     = {Interpolated estimation of Markov source parameters from sparse data},
  booktitle = {Proceedings of the Workshop on Pattern Recognition in Practice},
  volume    = {2},
  pages     = {381--397},
  year      = {1980}
}

@article{rabiner1989,
  author    = {Rabiner, L. R.},
  title     = {A tutorial on hidden Markov models and selected applications in speech recognition},
  journal   = {Proceedings of the IEEE},
  volume    = {77},
  number    = {2},
  pages     = {257--286},
  year      = {1989},
  doi       = {10.1109/5.18626}
}

@article{berger1996,
  author    = {Berger, A. and Della Pietra, S. A. and Della Pietra, V. J.},
  title     = {A maximum entropy approach to natural language processing},
  journal   = {Computational Linguistics},
  volume    = {22},
  number    = {1},
  pages     = {39--71},
  year      = {1996}
}

@article{grossberg2013,
  author    = {Grossberg, S.},
  title     = {Recurrent neural networks},
  journal   = {Scholarpedia},
  volume    = {8},
  number    = {2},
  pages     = {1888},
  year      = {2013}
}

@article{elman1990,
  author    = {Elman, J. L.},
  title     = {Finding structure in time},
  journal   = {Cognitive Science},
  volume    = {14},
  number    = {2},
  pages     = {179--211},
  year      = {1990}
}

@article{ster2013,
  author    = {\v{S}ter, B.},
  title     = {Selective recurrent neural network},
  journal   = {Neural Processing Letters},
  volume    = {38},
  pages     = {1--15},
  year      = {2013}
}


@article{wei2022finetuned,
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  title = {Finetuned language models are zero-shot learners},
  journal = {arXiv preprint arXiv:2109.01652},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2109.01652},
}


@inproceedings{brants2007large,
  author = {Brants, Thorsten and Popat, Ashok C. and Xu, Peng and Och, Franz J. and Dean, Jeffrey},
  title = {Large language models in machine translation},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year = {2007},
}


@misc{bubeck2023sparks,
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  title = {Sparks of artificial general intelligence: Early experiments with GPT-4},
  year = {2023},
  howpublished = {\url{http://arxiv.org/abs/2303.12712}},
  note = {arXiv preprint arXiv:2303.12712},
}


@inproceedings{coppersmith2015adhd,
  author = {Coppersmith, Glen and Dredze, Mark and Harman, Craig and Hollingshead, Kristy},
  title = {From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses},
  booktitle = {Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality},
  year = {2015},
  pages = {1--10},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado},
  doi = {https://doi.org/10.3115/v1/W15-1201},
}


@article{kruzan2022social,
  author = {Kruzan, Kaylee Payne and Williams, Kofoworola D. A. and Meyerhoff, Jonah and Yoo, Dong Whi and O'Dwyer, Linda C. and De Choudhury, Munmun and Mohr, David C.},
  title = {Social media-based interventions for adolescent and young adult mental health: A scoping review},
  journal = {Internet Interventions},
  volume = {30},
  year = {2022},
  pages = {100578},
  doi = {https://doi.org/10.1016/j.invent.2022.100578},
}


@misc{mha2022state,
  author = {Mental Health America},
  title = {The state of mental health in America},
  year = {2022},
  howpublished = {\url{https://mhanational.org/issues/state-mental-health-america}},
}

@misc{nimh2023mental,
  author = {National Institute of Mental Health},
  title = {Mental illness},
  year = {2023},
  howpublished = {\url{https://www.nimh.nih.gov/health/statistics/mental-illness}},
}


@misc{nami2023mental,
  author = {National Alliance on Mental Illness},
  title = {Mental health by the numbers},
  year = {2023},
  howpublished = {\url{https://nami.org/mhstats}},
}


@inproceedings{sarkar2022predicting,
  author = {Sarkar, Shailik and Alhamadani, Abdulaziz and Alkulaib, Lulwah and Lu, Chang-Tien},
  title = {Predicting depression and anxiety on Reddit: A multi-task learning approach},
  booktitle = {2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
  year = {2022},
  pages = {427--435},
  publisher = {IEEE},
  doi = {10.1109/ASONAM55676.2022.00066},
}


@inproceedings{cameron2017towards,
  author = {Cameron, Gillian and Cameron, David and Megaw, Gavin and Bond, Raymond and Mulvenna, Maurice and O’Neill, Siobhan and Armour, Cherie and McTear, Michael},
  title = {Towards a chatbot for digital counselling},
  booktitle = {Proceedings of the HCI 2017 Conference},
  year = {2017},
  doi = {10.14236/ewic/HCI2017.24},
  url = {https://doi.org/10.14236/ewic/HCI2017.24},
}

@article{abd-alrazaq2019overview,
  author = {Abd-alrazaq, Alaa A. and Alajlani, Mohannad and Alalwan, Ali Abdallah and Bewick, Bridgette M. and Gardner, Peter and Househ, Mowafa},
  title = {An overview of the features of chatbots in mental health: A scoping review},
  journal = {International Journal of Medical Informatics},
  volume = {132},
  pages = {103978},
  year = {2019},
  doi = {10.1016/j.ijmedinf.2019.103978},
  url = {https://doi.org/10.1016/j.ijmedinf.2019.103978},
}



@article{chen2019can,
  author = {Chen, Irene Y. and Szolovits, Peter and Ghassemi, Marzyeh},
  title = {Can AI help reduce disparities in general medical and mental health care?},
  journal = {AMA Journal of Ethics},
  volume = {21},
  number = {2},
  pages = {E167--E179},
  year = {2019},
  doi = {10.1001/amajethics.2019.167},
  url = {https://doi.org/10.1001/amajethics.2019.167},
}


@misc{devlin2019bert,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year = {2019},
  howpublished = {\url{http://arxiv.org/abs/1810.04805}},
  note = {arXiv:1810.04805 [cs]},
}


@article{omar2023chatgpt,
  author = {Omar, Reham and Mangukiya, Omij and Kalnis, Panos and Mansour, Essam},
  title = {ChatGPT versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots},
  journal = {arXiv preprint arXiv:2302.06466},
  year = {2023},
}

@article{jiang2023health,
  author = {Jiang, Lavender Yao and Liu, Xujin Chris and Nejatian, Nima Pour and Nasir-Moin, Mustafa and Wang, Duo and Abidin, Anas and Eaton, Kevin and Riina, Howard Antony and Laufer, Ilya and Punjabi, Paawan and Miceli, Madeline and Kim, Nora C. and Orillac, Cordelia and Schnurman, Zane and Livia, Christopher and Weiss, Hannah and Kurland, David and Neifert, Sean and Dastagirzada, Yosef and Kondziolka, Douglas and Cheung, Alexander T. M. and Yang, Grace and Cao, Ming and Flores, Mona and Costa, Anthony B. and Aphinyanaphongs, Yindalon and Cho, Kyunghyun and Oermann, Eric Karl},
  title = {Health system-scale language models are all-purpose prediction engines},
  journal = {Nature},
  year = {2023},
  month = {June},
  doi = {10.1038/s41586-023-06160-y},
  url = {https://doi.org/10.1038/s41586-023-06160-y},
}


@article{singhal2023towards,
  author = {Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and Schaekermann, Mike and Wang, Amy and Amin, Mohamed and Lachgar, Sami and Mansfield, Philip and Prakash, Sushant and Green, Bradley and Dominowska, Ewa and Aguera y Arcas, Blaise and Tomasev, Nenad and Liu, Yun and Wong, Renee and Semturs, Christopher and Mahdavi, S. Sara and Barral, Joelle and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Azizi, Shekoofeh and Karthikesalingam, Alan and Natarajan, Vivek},
  title = {Towards expert-level medical question answering with large language models},
  journal = {arXiv preprint arXiv:2305.09617},
  year = {2023},
  url = {http://arxiv.org/abs/2305.09617},
  doi = {10.48550/arXiv.2305.09617},
}


@article{benton2017multitask,
  author = {Benton, Adrian and Mitchell, Margaret and Hovy, Dirk},
  title = {Multi-task learning for mental health using social media text},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.03538},
  howpublished = {\url{https://arxiv.org/abs/1712.03538}},
}

@inproceedings{jo2023understanding,
  author = {Jo, Eunkyung and Epstein, Daniel A. and Jung, Hyunhoon and Kim, Young-Ho},
  title = {Understanding the benefits and challenges of deploying conversational AI leveraging large language models for public health intervention},
  booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages = {1--16},
  year = {2023},
}


@article{lamichhane2023evaluation,
  author = {Lamichhane, Bishal},
  title = {Evaluation of ChatGPT for NLP-based mental health applications},
  journal = {arXiv preprint arXiv:2303.15727},
  year = {2023},
  url = {http://arxiv.org/abs/2303.15727},
}


@article{amin2023affective,
  author = {Amin, Mostafa M. and Cambria, Erik and Schuller, Bj{\"o}rn W.},
  title = {Will affective computing emerge from foundation models and general AI? A first evaluation on ChatGPT},
  journal = {arXiv preprint arXiv:2303.03186},
  year = {2023},
  url = {http://arxiv.org/abs/2303.03186},
}


@article{yang2023evaluations,
  author = {Yang, Kailai and Ji, Shaoxiong and Zhang, Tianlin and Xie, Qianqian and Ananiadou, Sophia},
  title = {On the evaluations of ChatGPT and emotion-enhanced prompting for mental health analysis},
  journal = {arXiv preprint arXiv:2304.03347},
  year = {2023},
  url = {http://arxiv.org/abs/2304.03347},
}


@article{yang2023mental,
  author = {Yang, Kailai and Zhang, Tianlin and Kuang, Ziyan and Xie, Qianqian and Ananiadou, Sophia and Huang, Jimin},
  title = {MentaLLaMA: Interpretable mental health analysis on social media with large language models},
  journal = {arXiv preprint arXiv:2309.13567 [cs]},
  year = {2023},
  url = {http://arxiv.org/abs/2309.13567},
}


@article{Vaswani2017,
  author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, Ł. and Polosukhin, I.},
  title = {Attention is all you need},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017},
  url = {https://arxiv.org/abs/1706.03762}
}

@article{Radford2018,
  author = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
  title = {Improving language understanding by generative pre-training},
  year = {2018},
  journal = {Open AI Blog},
  url = {https://openai.com/research/language-unsupervised}
}

@inproceedings{Strubell2018,
  author = {Strubell, E. and Ganesh, A. and McCallum, A.},
  title = {Energy and policy considerations for deep learning in NLP},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2018)},
  year = {2018},
  pages = {3645--3650},
  url = {https://aclanthology.org/P18-1099/}
}

@inproceedings{Raganato2018,
  author = {Raganato, A. and Tiedemann, J.},
  title = {Analysis of Encoder Representations in Transformer-Based Machine Translation},
  booktitle = {Proceedings of the Third Conference on Machine Translation (WMT 2018)},
  year = {2018},
  pages = {108--118},
  url = {https://www.aclweb.org/anthology/W18-6312}
}

@inproceedings{Voita2019b,
  author = {Voita, E. and Wieting, J. and Firat, O. and Lin, X. and Johnson, M.},
  title = {On the (Un)Translatability of Encoder Representations},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019)},
  year = {2019},
  pages = {3814--3825},
  url = {https://arxiv.org/abs/1908.01500}
}

@inproceedings{Hao2019a,
  author = {Hao, Y. and Lu, Z. and Liu, J. and Xu, W.},
  title = {A Survey of Transformer-based Models for Natural Language Processing},
  booktitle = {Proceedings of the 2019 Conference on Natural Language Processing (NLP 2019)},
  year = {2019},
  pages = {158--169},
  url = {https://arxiv.org/abs/1906.07817}
}

@inproceedings{Shaw2018,
  author = {Shaw, P. and Uszkoreit, J. and Vaswani, A.},
  title = {Self-attention with relative position representations},
  booktitle = {Proceedings of the 2018 Conference on Neural Information Processing Systems (NeurIPS 2018)},
  year = {2018},
  pages = {4642--4650},
  url = {https://arxiv.org/abs/1803.02155}
}

@inproceedings{Yang2018,
  author = {Yang, Z. and Dai, A. M. and Yang, Y. and Salakhutdinov, R. and Cohen, W. W.},
  title = {Attention is not Explanation},
  booktitle = {Proceedings of the 2018 Conference on Neural Information Processing Systems (NeurIPS 2018)},
  year = {2018},
  url = {https://arxiv.org/abs/1803.07337}
}

@inproceedings{Wu2019,
  author = {Wu, Z. and Shen, Y. and Zhang, L. and Liu, T. and Wang, X.},
  title = {Convolutional Networks for NLP with Dynamic Receptive Fields},
  booktitle = {Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)},
  year = {2019},
  pages = {2155--2162},
  url = {https://arxiv.org/abs/1903.09089}
}

@inproceedings{Cui2019,
  author = {Cui, Y. and Liu, P. and Wei, F. and Zhou, M. and Wang, H.},
  title = {A Survey on Transformer Models in Natural Language Processing},
  booktitle = {Proceedings of the 2019 Conference on Natural Language Processing (NLP 2019)},
  year = {2019},
  pages = {134--145},
  url = {https://arxiv.org/abs/1908.01813}
}


@inproceedings{garg2021transformers,
  title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  author={Garg, S. and Tsipras, D. and Liang, P. and Valiant, G.},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML 2021)},
  pages={3505--3514},
  year={2021},
  url={https://arxiv.org/abs/2104.01561},
  note={Accessed: 12 March 2025}
}

@inproceedings{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J.D.},
  booktitle={Proceedings of NeurIPS 2020},
  pages={1877--1901},
  year={2020},
  url={https://arxiv.org/abs/2005.14165},
  note={Accessed: 12 March 2025}
}

@inproceedings{wang2022self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, X. and Wei, J. and Schuurmans, D. and Le, Q. and Chi, E.},
  booktitle={Proceedings of NeurIPS 2022},
  pages={3442--3453},
  year={2022},
  url={https://arxiv.org/abs/2203.16346},
  note={Accessed: 12 March 2025}
}

@inproceedings{yao2023tree,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author={Yao, S. and Yu, D. and Zhao, J. and Shafran, I. and Griffiths, T.L.},
  booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML 2023)},
  pages={3671--3682},
  year={2023},
  url={https://arxiv.org/abs/2302.05732},
  note={Accessed: 12 March 2025}
}

@inproceedings{yao2022react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, S. and Zhao, J. and Yu, D. and Shafran, I. and Griffiths, T.L.},
  booktitle={Proceedings of NeurIPS 2022},
  pages={4562--4573},
  year={2022},
  url={https://arxiv.org/abs/2204.08670},
  note={Accessed: 12 March 2025}
}


