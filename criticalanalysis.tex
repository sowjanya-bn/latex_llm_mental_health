In the development of a mental health chatbot designed to assist individuals experiencing depression, there were several limitations and challenges due to the use of a free platform, the short timeframe of the project (12 weeks), and the complexity of designing an AI-driven tool for such sensitive use cases. This section critically evaluates the chatbot’s performance, highlighting its strengths, limitations, and areas for potential improvement.
\subsection*{Limitations of the Free Platform} 
The decision to use a free platform to build the chatbot imposed significant constraints on its functionality and development. Free platforms typically offer limited resources compared to paid services, which can impact both the scale and quality of the final product. These platforms often restrict access to advanced model features, such as fine-tuning and customisation, and have fewer capabilities for managing large datasets.
For instance, a paid service might provide the ability to train the model on larger and more specific datasets, particularly those related to mental health, which would enhance the chatbot’s ability to provide contextually appropriate and empathetic responses. The limited functionalities of the free platform used in this project prevented the implementation of advanced features such as dynamic backpropagation, which is essential for improving the chatbot’s understanding of context and refining its output over time. Further, the created was built off of a chatbot with generic, and limited training data, meaning that our ability to fine tune this to the required specifications added another layer of complexity.
\subsection*{Data Collection and Model Training Challenges} 
Mental health chatbots require specialized training data that includes therapeutic dialogue, expert-curated mental health advice, and crisis intervention scripts. However, without access to a robust dataset, the chatbot may struggle to provide meaningful and helpful responses. In particular, the chatbot is likely to give generic answers that lack the nuance needed to address the unique experiences of individuals with depression. Furthermore, without fine-tuning the model on mental health-specific data, the chatbot may fail to recognise subtle cues or signs of a more severe mental health crisis, such as suicidal ideation, which could potentially jeopardise the user’s safety.
\subsection*{Ethical and Safety Risks} 
The ethical considerations in developing a mental health chatbot are paramount, especially when the chatbot is intended to engage with vulnerable individuals. In its current form, the chatbot is prone to several ethical risks, particularly around its handling of sensitive topics like suicide and self-harm. A major concern is the chatbot’s tendency to introduce these topics prematurely, even when they have not been mentioned by the user. Such responses can exacerbate distress and disengage users, potentially escalating the situation.
The lack of safety mechanisms in the current model makes it difficult to provide a supportive and responsive environment for users in crisis. In more advanced systems, mechanisms like sentiment analysis, emergency alerts, and crisis detection would allow the chatbot to identify when a user may be in immediate danger and refer them to appropriate resources. Without these safety features, the chatbot’s current use in mental health contexts is problematic, as it may inadvertently cause harm by misinterpreting the user’s emotional state or failing to escalate critical situations when necessary.
\subsection*{Personalisation and Continuity of Care} 
One of the core challenges of the chatbot is its lack of personalisation. Depression is a deeply personal and complex mental health condition, and effective therapeutic tools must be able to adapt to the individual needs of each user. The free platform’s limitations hinder the development of such personalised features, as the chatbot currently struggles to maintain continuity in conversations or respond dynamically to the evolving emotional state of the user.
In a more advanced system, the chatbot would be able to learn from prior interactions with the user, adjusting its tone and responses to better match the user’s emotional context. For example, if the chatbot detects that a user has been expressing consistent feelings of hopelessness or isolation, it could provide more targeted advice or escalate the conversation to a higher level of support. The ability to tailor responses based on the individual’s prior inputs is critical in maintaining user engagement and ensuring the chatbot provides ongoing, meaningful support.
\subsection*{Potential Improvements and Future Directions} 
To enhance the chatbot’s effectiveness, several improvements should be considered in future iterations. Upgrading to a paid platform would allow the chatbot to take advantage of more advanced features, such as model fine-tuning and access to larger, more specialised datasets. With these resources, the chatbot would be able to produce more accurate, contextually relevant, and empathetic responses, particularly in sensitive areas like mental health.
Moreover, to improve the model’s performance, it is essential to collect more diverse and specialised training data. Collaborating with mental health professionals to curate high-quality datasets, such as therapeutic dialogues, expert-curated resources, and crisis intervention scripts, would ensure the chatbot’s responses align with established mental health frameworks. This data would provide the foundation for fine-tuning the model, enabling it to recognise a broader range of emotional cues and handle more complex mental health issues with greater sensitivity.
Additionally, incorporating robust safety features is critical. Future versions of the chatbot should be designed with crisis detection mechanisms that can flag critical emotional states, such as suicidal ideation or severe distress, and trigger appropriate responses. These might include providing emergency contact information or automatically escalating the issue to a mental health professional. Integrating sentiment analysis would further enhance the chatbot’s ability to gauge the emotional tone of conversations, allowing it to respond more sensitively to the user’s state.
Personalisation is another key area for improvement. By allowing the chatbot to track and build upon previous conversations, it could provide more individualised support. The chatbot could adapt its responses based on the user’s unique emotional context and history, offering a more tailored approach to mental health care. This personalised engagement would be crucial in fostering a sense of connection and support, especially for users who may feel isolated or disconnected.
Finally, collaboration with mental health experts throughout the development and fine-tuning stages would ensure that the chatbot adheres to therapeutic guidelines and provides safe, effective support. This input would be invaluable in helping to align the chatbot’s responses with best practices in mental health care, ensuring that it offers both practical advice and compassionate, evidence-based guidance.