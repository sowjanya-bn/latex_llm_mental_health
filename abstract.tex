Numerous applications have been made possible by developments in large language models (LLMs). Research on comprehending and improving the skills of LLMs in the field of mental health is still lacking, though. In this study, we provide a thorough assessment of several LLMs on a range of mental health prediction tasks using online text data, such as Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. Our experiments include a wide range of topics, including instruction fine-tuning, zero-shot prompting, and few-shot prompting. For mental health tasks, the results show that LLMs with zero-shot and few-shot prompt designs perform promisingly but limitedly. More importantly, we have demonstrated through experiments that instruction finetuning can greatly improve LLM performance across all tasks at once.

To improve LLMs' performance on mental health tasks, we summarise our results into a series of action recommendations. We also stress the significant constraints, like documented racial and gender prejudice, that must be overcome before deployability in actual mental health settings can be achieved. We bring attention to the significant ethical and privacy concerns that come with this possibilities of research.