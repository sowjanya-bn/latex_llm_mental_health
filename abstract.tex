This study presents a comprehensive review of the capabilities, limitations, and ethical implications of using large language models (LLMs) in mental health therapeutics. We explore the evolution of LLMs, from early models like ELIZA to modern architectures such as GPT-4 and FLAN-T5, and evaluate their effectiveness across mental health prediction and support tasks including stress, depression and suicide risk detection. Drawing on fine-tuning strategies, zero- and few-shot prompting and instruction learning, the study analyzes how models such as Alpaca, FLAN-T5 and GPT variants perform using benchmark datasets like Reddit Suicide Watch and EmpatheticDialogues. We highlight the trade-offs between general-purpose and fine-tuned models in accuracy, empathy, and real-time responsiveness. Special focus is given to the privacy, ethical and safety risks inherent in deploying AI chatbots in clinical settings. The study concludes with actionable recommendations for prompt engineering, dataset curation, personalization and crisis response safeguards to improve the reliability of LLMs for mental health applications.