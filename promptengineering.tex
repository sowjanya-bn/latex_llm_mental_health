\subsection{Introduction to Prompt Engineering} 
Prompt engineering refers to the deliberate design and manipulation of the inputs provided to large language models (LLMs) in order to optimise performance across various tasks. The body of research reviewed below collectively highlights that the structure, clarity, and contextual richness of a prompt significantly affect an LLM’s ability to generalise, reason, and generate high-quality responses. A critical analysis of these studies reveals an evolving understanding of how prompt structure influences LLM outputs, not only in simple tasks but also in complex, reasoning-intensive scenarios, offering key implications for their use in sensitive applications such as mental health support.

\subsection{Prompt Clarity and Task Generalisation} 
\cite{garg2021transformers} examine the in-context learning capabilities of transformer-based models, focusing on their ability to perform simple functions like sorting and pattern recognition. Their findings underscore that prompt structure is particularly crucial for enabling generalisation in tasks that fall just beyond training data, what they call "medial" tasks. Notably, as task complexity increases, the model's performance degrades unless the prompt is structured to support more abstract reasoning. This introduces an important insight: while LLMs can simulate learning through prompt engineering, this ability is fragile and highly dependent on how well the prompt scaffolds the task \cite{garg2021transformers}.
\cite{brown2020language} extend this idea through the concept of few-shot learning, showing that providing clear, structured examples within prompts dramatically improves performance. They find that including multiple relevant task examples (as opposed to zero- or one-shot prompts) increases contextual understanding and reduces ambiguity. Together, these studies suggest that prompt engineering is not just about making inputs readable, it is about making the task learnable within the constraints of the LLM’s architecture \cite{brown2020language}. This has direct implications for mental health chatbots, where ambiguous user input must be interpreted with nuance and sensitivity.


\subsection{Advanced Prompting strategies: Reasoning and Reflection} 
While Garg and Brown focus on prompt clarity and structure in general task completion, more recent work by \cite{wang2022self} and \cite{yao2023tree} pushes this further by exploring how prompting can shape the reasoning process of LLMs. \cite{wang2022self} Chain-of-Thought (CoT) prompting introduces intermediate reasoning steps that mimic human-like deliberation. Their results show that CoT prompting not only improves task performance but also maintains accuracy as task complexity increases, a contrast to \cite{garg2021transformers} finding that LLM performance diminishes under complexity. This suggests that introducing a structured reasoning process within prompts can compensate for limitations in generalisation, further supporting the need for more sophisticated prompt strategies \cite{wang2022self}.
Building on this, \cite{yao2023tree} propose Tree-of-Thought (ToT) prompting, which enables the model to explore multiple solution paths before selecting the most appropriate one. Unlike CoT, which tends to follow a linear reasoning path, ToT encourages branching thought processes and reflective evaluation. This aligns with decision-making frameworks used in cognitive-behavioural therapy (CBT) and other mental health contexts, where multiple perspectives and reflective thinking are essential. The findings from ToT experiments demonstrate that LLMs can be prompted not just to answer questions, but to think through them in a structured, human-like way, critical for engaging users in therapeutic dialogue \cite{yao2023tree}.

\subsection{Reasoning-Action Separation in Interactive Tasks} 
Further refining this approach, \cite{yao2022react} introduce ReAct prompting, which separates reasoning from action. Here, the model first reasons through the problem, then generates a final output. This division ensures that responses are not just reactive but deliberative, enhancing LLM performance in interactive tasks like question-answering. In the context of mental health, this separation is particularly important: it mirrors the therapeutic process of validating emotions before suggesting actions, a key to empathetic engagement \cite{yao2022react}.

\subsection{Evolving Understanding of Prompt Engineering} 
Taken together, these studies highlight a central inference: high-quality LLM outputs are not merely a function of the model’s size or training data, but of how intelligently they are prompted. While early research like that of \cite{garg2021transformers, brown2020language} emphasised prompt clarity and example inclusion, newer approaches like CoT, ToT, and ReAct show that prompting can simulate cognitive processes such as reflection, evaluation, and decision-making. This progression reflects a broader shift in prompt engineering, from focusing solely on inputs to shaping the model’s thinking process.


\subsection{Implications for Mental Health Chatbots}
These findings carry profound implications for the design of LLM-based mental health chatbots. Individuals in psychological distress often struggle to express themselves clearly or to provide structured input. This makes user-initiated prompt quality highly variable. If the chatbot relies solely on direct user input, its responses risk being generic, inappropriate, or even harmful.
To address this, the chatbot must act as an intermediary prompt engineer, restructuring, expanding, or interpreting user inputs using internal reasoning frameworks. Techniques such as CoT and ReAct can be embedded into the system to simulate deeper understanding, allowing the model to "think through" user concerns before responding \cite{wang2022self, yao2022react}. For example, instead of responding directly to “I feel off,” the model could internally generate a series of interpretive questions, reflections, and emotional validations before offering a meaningful reply.
Additionally, the insights from ToT prompting can be used to generate multiple potential responses and select the one that best aligns with therapeutic goals, such as validation, reflection, or gentle guidance \cite{yao2023tree}. In this way, the chatbot mimics the thoughtful deliberation of a trained therapist, increasing the likelihood of meaningful engagement.
Finally, prompt engineering becomes not just a tool for improving performance but a safeguard. By structuring internal reasoning processes, LLMs can avoid surface-level interpretations and respond in ways that are more aligned with therapeutic best practices, improving both the emotional and clinical efficacy of the chatbot.